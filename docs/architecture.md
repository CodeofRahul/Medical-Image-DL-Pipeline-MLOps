# 🏗️ Project Architecture & Folder Structure Guide

This document explains the standard structure of a Machine Learning (ML), Deep Learning (DL), or Natural Language Processing (NLP) project aligned with **MLOps best practices**.\
It serves both beginners trying to understand folder purposes and experienced developers looking for consistency.

---

## 📂 High-Level Folder & File Structure

```
├── config/
│   └── config.yaml
├── src/
│   ├── data/
│   │   └── data_ingestion.py
│   ├── features/
│   │   └── feature_engineering.py
│   ├── models/
│   │   └── model_trainer.py
│   ├── pipelines/
│   │   └── training_pipeline.py
│   ├── utils/
│   │   └── common.py
│   └── config_loader.py
├── notebooks/
│   └── data_exploration.ipynb
├── data/
├── artifacts/
├── models/
├── tests/
│   └── test_data_ingestion.py
├── docker/
│   └── Dockerfile
├── deployment/
│   └── k8s_deployment.yaml
├── monitoring/
│   └── prometheus_config.yaml
├── logs/
├── .github/workflows/
│   └── ci_cd_pipeline.yaml
├── requirements.txt / pyproject.toml
├── dvc.yaml / params.yaml
├── README.md
├── Makefile
└── docs/
    └── architecture.md
```

---

## 🔥 Folder & File Responsibilities

### 1️⃣ `config/` — **Centralized Configuration Management**

- Contains `` — Defines static configurations like file paths, hyperparameters, model save paths, experiment names.
- Keeps the code **config-driven** and avoids hardcoding.

Example:

```yaml
data_ingestion:
  root_dir: artifacts/data_ingestion
  source_url: https://dataset.com/data.zip
  unzip_dir: artifacts/data_ingestion/unzipped

model_training:
  root_dir: artifacts/model_training
  epochs: 10
  learning_rate: 0.001
```


---

### 2️⃣ `src/` — **Source Code (Modularized by Functionality)**

`src/data/data_ingestion.py`

```
def download_data(url: str, save_path: str):
    # Code to download data from URL
    pass
```

`src/features/feature_engineering.py`

```
def clean_text(text: str) -> str:
    return text.lower().strip()
```

`src/models/model_trainer.py`

```
from sklearn.linear_model import LogisticRegression

def train_model(X_train, y_train):
    model = LogisticRegression()
    model.fit(X_train, y_train)
    return model
```

`src/pipelines/training_pipeline.py`

```
from src.data.data_ingestion import download_data
from src.features.feature_engineering import clean_text
from src.models.model_trainer import train_model

def run_training_pipeline():
    # Steps to orchestrate entire ML workflow
    pass
```

`src/utils/common.py`

```
import logging

def get_logger(name):
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    return logger
```

`src/config_loader.py`

```
import yaml

def load_config(config_path: str):
    with open(config_path) as f:
        return yaml.safe_load(f)
```


Recommended subfolders inside `src/`:

| Subfolder          | Responsibility                                                   |
| ------------------ | ---------------------------------------------------------------- |
| `data/`            | Data ingestion, downloading, loading, splitting                  |
| `features/`        | Feature engineering, preprocessing                               |
| `models/`          | Model architecture, training, saving                             |
| `pipelines/`       | Complete pipeline definitions for training, evaluation           |
| `utils/`           | Utility functions (logging, common helpers)                      |
| `config_loader.py` | Reads YAML, loads into typed classes (e.g., using `dataclasses`) |

> 💡 **Key Rule:** Each module handles **one responsibility only** for better scalability.

---

### 3️⃣ `notebooks/` — **Exploration & Experimentation**

- Jupyter notebooks for **EDA**, initial experiments, model trials.
- Should be cleaned periodically — **not production code**.

```
# notebooks/data_exploration.ipynb
# Used for initial data exploration and visualization
```

---

### 4️⃣ `data/` — **Local Data Storage (Optional, DVC recommended)**

- Holds datasets if not using DVC.
- Use `data/raw/`, `data/processed/` convention.

```
Contains raw and processed data
```

> ⚠️ **Never commit large datasets to Git.**

---

### 5️⃣ `artifacts/` — **Generated Files from Pipeline Runs**

- Holds outputs of pipeline stages like:
  - Raw downloaded data
  - Processed data
  - Trained models
  - Metrics reports
- Clean this before retraining if needed.

```
Auto-generated by pipeline runs, contains models, logs, etc.
```

---

### 6️⃣ `models/` — **Saved Model Artifacts**

- For finalized model artifacts for serving or registry.

```
Finalized models ready for serving
```

---

### 7️⃣ `tests/` — **Unit Tests & Integration Tests**

- Test files following `test_*.py` convention.
- Use frameworks like `pytest`.
- Covers:
  - Data loader tests
  - Model training functions
  - Utility functions

```
# tests/test_data_ingestion.py

def test_download_data():
    # Write your test cases here
    assert True
```

---

### 8️⃣ `docker/` — **Dockerization Files**

- Dockerfile(s)
- Docker Compose files
- Entrypoint scripts if needed

```
# docker/Dockerfile
FROM python:3.10
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
CMD ["python", "src/pipelines/training_pipeline.py"]
```

---

### 9️⃣ `deployment/` — **Deployment-Related Files**

- Kubernetes manifests
- Helm charts
- AWS CloudFormation / Terraform scripts
- Inference service configs

```
# deployment/k8s_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-app-deployment
```

---

### 🔟 `monitoring/` — **Monitoring & Observability**

- Prometheus configs
- Grafana dashboards
- Alertmanager rules

```
# monitoring/prometheus_config.yaml
scrape_configs:
  - job_name: 'ml_app'
```

---

### 1️⃣1️⃣ `.github/workflows/` — **CI/CD Pipelines**

- YAML files for GitHub Actions
- Automate:
  - Testing
  - Building Docker images
  - Deployment to servers

```
# .github/workflows/ci_cd_pipeline.yaml
name: CI/CD
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
```

---

### 1️⃣2️⃣ `logs/` — **Logs (If Managed Locally)**

- Logs from training, evaluation, serving
- Can be managed by loggers like `loguru` or `python-logging`

`Stores training logs, evaluation metrics, etc.`

---

### 1️⃣3️⃣ `Makefile` — **Task Automation (Optional but Useful)**

- Common commands like:

```makefile
setup:
    pip install -r requirements.txt

train:
    python src/train_pipeline.py
```

---

### 1️⃣4️⃣ `README.md` — **Project Overview & Setup Guide**

- Should explain:
  - Project purpose
  - Setup steps
  - Usage instructions
  - Contribution guidelines

```
Explains project purpose, installation steps, usage, and contributing guidelines
```

---

### 1️⃣5️⃣ `dvc.yaml` & `params.yaml` — **Data Versioning & Experiment Tracking**

- `dvc.yaml` defines stages of data pipelines.
- `params.yaml` defines experiment parameters like learning rates.
- Keeps your experiments **reproducible**.

```
# params.yaml
model:
  learning_rate: 0.001

# dvc.yaml
stages:
  prepare_data:
    cmd: python src/data/data_ingestion.py
    outs:
      - data/processed/
```

---

## ✅ Recommended Tool Stack per Layer

| Layer               | Tools                    |
| ------------------- | ------------------------ |
| Data Versioning     | DVC                      |
| Experiment Tracking | MLflow, Weights & Biases |
| Model Registry      | MLflow Model Registry    |
| Containerization    | Docker                   |
| CI/CD               | GitHub Actions, Jenkins  |
| Deployment          | Kubernetes, AWS EKS      |
| Monitoring          | Prometheus, Grafana      |

---

## ✅ General Best Practices

- ✅ Keep **business logic, configs, and pipelines separate**.
- ✅ Use **data classes** to load configs from YAML.
- ✅ Avoid hardcoding — read from config files.
- ✅ Make use of logging instead of `print()`.
- ✅ Write unit tests for critical functions.
- ✅ Use CI/CD for automation.
- ✅ Use Docker for consistent environments.
- ✅ Use DVC for data & model versioning.
- ✅ Track experiments using MLflow.

---

## ✅ Example Config Flow

```
config/config.yaml  →  src/config_loader.py  →  src/pipelines/training_pipeline.py
```

- `` stores static paths and params.
- `` reads YAML and converts it into Python objects.
- `` uses these objects to perform actual training.

---

## ✅ Folder Cleanup Reminder (Optional for Production)

- 🔥 Clean `notebooks/` after EDA phase.
- 🗑️ Clean `artifacts/` before reruns if outdated.
- 🗑️ Clean `logs/` periodically.
- 🗑️ Never push large datasets or sensitive info.

---

## 📝 Final Note

> "Your folder structure is your project’s silent documentation."\
> Keep it clean, modular, and well-documented. It saves hours of confusion — for both you and your team.

