# ðŸ—ï¸ Project Architecture & Folder Structure Guide

This document explains the standard structure of a Machine Learning (ML), Deep Learning (DL), or Natural Language Processing (NLP) project aligned with **MLOps best practices**.\
It serves both beginners trying to understand folder purposes and experienced developers looking for consistency.

---

## ðŸ“‚ High-Level Folder & File Structure

```
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ common.py
â”‚   â””â”€â”€ config_loader.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ data_exploration.ipynb
â”œâ”€â”€ data/
â”œâ”€â”€ artifacts/
â”œâ”€â”€ models/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_data_ingestion.py
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ deployment/
â”‚   â””â”€â”€ k8s_deployment.yaml
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ prometheus_config.yaml
â”œâ”€â”€ logs/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci_cd_pipeline.yaml
â”œâ”€â”€ requirements.txt / pyproject.toml
â”œâ”€â”€ dvc.yaml / params.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ Makefile
â””â”€â”€ docs/
    â””â”€â”€ architecture.md
```

---

## ðŸ”¥ Folder & File Responsibilities

### 1ï¸âƒ£ `config/` â€” **Centralized Configuration Management**

- Contains `` â€” Defines static configurations like file paths, hyperparameters, model save paths, experiment names.
- Keeps the code **config-driven** and avoids hardcoding.

Example:

```yaml
data_ingestion:
  root_dir: artifacts/data_ingestion
  source_url: https://dataset.com/data.zip
  unzip_dir: artifacts/data_ingestion/unzipped

model_training:
  root_dir: artifacts/model_training
  epochs: 10
  learning_rate: 0.001
```


---

### 2ï¸âƒ£ `src/` â€” **Source Code (Modularized by Functionality)**

`src/data/data_ingestion.py`

```
def download_data(url: str, save_path: str):
    # Code to download data from URL
    pass
```

`src/features/feature_engineering.py`

```
def clean_text(text: str) -> str:
    return text.lower().strip()
```

`src/models/model_trainer.py`

```
from sklearn.linear_model import LogisticRegression

def train_model(X_train, y_train):
    model = LogisticRegression()
    model.fit(X_train, y_train)
    return model
```

`src/pipelines/training_pipeline.py`

```
from src.data.data_ingestion import download_data
from src.features.feature_engineering import clean_text
from src.models.model_trainer import train_model

def run_training_pipeline():
    # Steps to orchestrate entire ML workflow
    pass
```

`src/utils/common.py`

```
import logging

def get_logger(name):
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    return logger
```

`src/config_loader.py`

```
import yaml

def load_config(config_path: str):
    with open(config_path) as f:
        return yaml.safe_load(f)
```


Recommended subfolders inside `src/`:

| Subfolder          | Responsibility                                                   |
| ------------------ | ---------------------------------------------------------------- |
| `data/`            | Data ingestion, downloading, loading, splitting                  |
| `features/`        | Feature engineering, preprocessing                               |
| `models/`          | Model architecture, training, saving                             |
| `pipelines/`       | Complete pipeline definitions for training, evaluation           |
| `utils/`           | Utility functions (logging, common helpers)                      |
| `config_loader.py` | Reads YAML, loads into typed classes (e.g., using `dataclasses`) |

> ðŸ’¡ **Key Rule:** Each module handles **one responsibility only** for better scalability.

---

### 3ï¸âƒ£ `notebooks/` â€” **Exploration & Experimentation**

- Jupyter notebooks for **EDA**, initial experiments, model trials.
- Should be cleaned periodically â€” **not production code**.

```
# notebooks/data_exploration.ipynb
# Used for initial data exploration and visualization
```

---

### 4ï¸âƒ£ `data/` â€” **Local Data Storage (Optional, DVC recommended)**

- Holds datasets if not using DVC.
- Use `data/raw/`, `data/processed/` convention.

```
Contains raw and processed data
```

> âš ï¸ **Never commit large datasets to Git.**

---

### 5ï¸âƒ£ `artifacts/` â€” **Generated Files from Pipeline Runs**

- Holds outputs of pipeline stages like:
  - Raw downloaded data
  - Processed data
  - Trained models
  - Metrics reports
- Clean this before retraining if needed.

```
Auto-generated by pipeline runs, contains models, logs, etc.
```

---

### 6ï¸âƒ£ `models/` â€” **Saved Model Artifacts**

- For finalized model artifacts for serving or registry.

```
Finalized models ready for serving
```

---

### 7ï¸âƒ£ `tests/` â€” **Unit Tests & Integration Tests**

- Test files following `test_*.py` convention.
- Use frameworks like `pytest`.
- Covers:
  - Data loader tests
  - Model training functions
  - Utility functions

```
# tests/test_data_ingestion.py

def test_download_data():
    # Write your test cases here
    assert True
```

---

### 8ï¸âƒ£ `docker/` â€” **Dockerization Files**

- Dockerfile(s)
- Docker Compose files
- Entrypoint scripts if needed

```
# docker/Dockerfile
FROM python:3.10
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
CMD ["python", "src/pipelines/training_pipeline.py"]
```

---

### 9ï¸âƒ£ `deployment/` â€” **Deployment-Related Files**

- Kubernetes manifests
- Helm charts
- AWS CloudFormation / Terraform scripts
- Inference service configs

```
# deployment/k8s_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-app-deployment
```

---

### ðŸ”Ÿ `monitoring/` â€” **Monitoring & Observability**

- Prometheus configs
- Grafana dashboards
- Alertmanager rules

```
# monitoring/prometheus_config.yaml
scrape_configs:
  - job_name: 'ml_app'
```

---

### 1ï¸âƒ£1ï¸âƒ£ `.github/workflows/` â€” **CI/CD Pipelines**

- YAML files for GitHub Actions
- Automate:
  - Testing
  - Building Docker images
  - Deployment to servers

```
# .github/workflows/ci_cd_pipeline.yaml
name: CI/CD
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
```

---

### 1ï¸âƒ£2ï¸âƒ£ `logs/` â€” **Logs (If Managed Locally)**

- Logs from training, evaluation, serving
- Can be managed by loggers like `loguru` or `python-logging`

`Stores training logs, evaluation metrics, etc.`

---

### 1ï¸âƒ£3ï¸âƒ£ `Makefile` â€” **Task Automation (Optional but Useful)**

- Common commands like:

```makefile
setup:
    pip install -r requirements.txt

train:
    python src/train_pipeline.py
```

---

### 1ï¸âƒ£4ï¸âƒ£ `README.md` â€” **Project Overview & Setup Guide**

- Should explain:
  - Project purpose
  - Setup steps
  - Usage instructions
  - Contribution guidelines

```
Explains project purpose, installation steps, usage, and contributing guidelines
```

---

### 1ï¸âƒ£5ï¸âƒ£ `dvc.yaml` & `params.yaml` â€” **Data Versioning & Experiment Tracking**

- `dvc.yaml` defines stages of data pipelines.
- `params.yaml` defines experiment parameters like learning rates.
- Keeps your experiments **reproducible**.

```
# params.yaml
model:
  learning_rate: 0.001

# dvc.yaml
stages:
  prepare_data:
    cmd: python src/data/data_ingestion.py
    outs:
      - data/processed/
```

---

## âœ… Recommended Tool Stack per Layer

| Layer               | Tools                    |
| ------------------- | ------------------------ |
| Data Versioning     | DVC                      |
| Experiment Tracking | MLflow, Weights & Biases |
| Model Registry      | MLflow Model Registry    |
| Containerization    | Docker                   |
| CI/CD               | GitHub Actions, Jenkins  |
| Deployment          | Kubernetes, AWS EKS      |
| Monitoring          | Prometheus, Grafana      |

---

## âœ… General Best Practices

- âœ… Keep **business logic, configs, and pipelines separate**.
- âœ… Use **data classes** to load configs from YAML.
- âœ… Avoid hardcoding â€” read from config files.
- âœ… Make use of logging instead of `print()`.
- âœ… Write unit tests for critical functions.
- âœ… Use CI/CD for automation.
- âœ… Use Docker for consistent environments.
- âœ… Use DVC for data & model versioning.
- âœ… Track experiments using MLflow.

---

## âœ… Example Config Flow

```
config/config.yaml  â†’  src/config_loader.py  â†’  src/pipelines/training_pipeline.py
```

- `` stores static paths and params.
- `` reads YAML and converts it into Python objects.
- `` uses these objects to perform actual training.

---

## âœ… Folder Cleanup Reminder (Optional for Production)

- ðŸ”¥ Clean `notebooks/` after EDA phase.
- ðŸ—‘ï¸ Clean `artifacts/` before reruns if outdated.
- ðŸ—‘ï¸ Clean `logs/` periodically.
- ðŸ—‘ï¸ Never push large datasets or sensitive info.

---

## ðŸ“ Final Note

> "Your folder structure is your projectâ€™s silent documentation."\
> Keep it clean, modular, and well-documented. It saves hours of confusion â€” for both you and your team.

